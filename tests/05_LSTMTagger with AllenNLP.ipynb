{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllenNLP \n",
    "## Must implement the following classes:\n",
    "1. DatasetReader\n",
    "2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.  DatasetReader\n",
    "\n",
    "* Must implement __init__\n",
    "  * must pass TokenIndexer(s) as argument to initialize\n",
    "* Must implemment _read method\n",
    "  * read files, http_pages etc\n",
    "  * pre-process the text \n",
    "* Must implement text_to_instance \n",
    "  * convert text into instances =[ Token, Label ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data import Token\n",
    "from allennlp.data import DatasetReader\n",
    "\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "\n",
    "from allennlp.common.file_utils import cached_path\n",
    "\n",
    "\n",
    "class PosDatasetReader(DatasetReader):\n",
    "   \n",
    "    def __init__(self,token_indexers:Dict[str,TokenIndexer]=None)->None:\n",
    "\n",
    "        \n",
    "        super().__init__(lazy=False)\n",
    "       \n",
    "        self.token_indexers = token_indexers or {\"tokens\":SingleIdTokenIndexer()}\n",
    "        \n",
    "    \n",
    "    def _read(self,file_path:str) -> Iterator[Instance]:\n",
    "\n",
    "        with open(file_path) as f :\n",
    "\n",
    "            for line in f:\n",
    "            \n",
    "                pairs = line.strip().split()\n",
    "                \n",
    "                sentence, tags = zip(*(pair.split(\"###\")for pair in pairs))\n",
    "                \n",
    "                print(\"sentence : \" , sentence , \" tags : \" , tags)\n",
    "                \n",
    "                yield self.text_to_instance([Token(word) for word in sentence],tags)\n",
    "                \n",
    "    \n",
    "    def text_to_instance(self,tokens: List[Token], tags: List[str]=None)->Instance:\n",
    "        \n",
    "        fields = dict()\n",
    "        \n",
    "        sentence_field = TextField(tokens,self.token_indexers)\n",
    "        \n",
    "        fields[\"sentence\"] = sentence_field # {\"sentence\":sentence_field}\n",
    " \n",
    "        # Make label fields if tag is given\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels = tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "                \n",
    "        # return instance list (= dataset)\n",
    "        return Instance(fields)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset \n",
    "\n",
    "# 1. Instantiate DatasetReader\n",
    "reader = PosDatasetReader()\n",
    "\n",
    "# 2. Call read to make dataset\n",
    "\n",
    "train_dataset = reader.read(cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/training.txt'))\n",
    "\n",
    "validation_dataset = reader.read(cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/validation.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Implement the Model\n",
    "\n",
    "* Sub-class of torch.nn.Module\n",
    "* Needs a forward method\n",
    "\n",
    "  * Must return a dict of tensor outputs\n",
    "  * Must include dict[loss] for training\n",
    "\n",
    "* In our model, we'll use : \n",
    "  * embedding layer\n",
    "  * sequence embedder (lstm)\n",
    "  * feedforward net\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "\n",
    "from allennlp.modules import TextFieldEmbedder, Seq2SeqEncoder\n",
    "\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "\n",
    "class LstmTagger(Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 word_embeddings : TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        \n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        self.word_embeddings = word_embeddings\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.hidden2tag = torch.nn.Linear(in_features  = encoder.get_output_dim(),\n",
    "                                          out_features = vocab.get_vocab_size('labels'))\n",
    "        \n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    \n",
    "    def forward(self,\n",
    "                sentence: Dict[str,torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> Dict[str,torch.Tensor]:\n",
    "        \n",
    "        mask = get_text_field_mask(sentence)\n",
    "        \n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        \n",
    "        encoder_out = self.encoder(embeddings,mask)\n",
    "        \n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        \n",
    "        output = {\"tag_logits\":tag_logits}\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output['loss']=sequence_cross_entropy_with_logits(tag_logits,labels,mask)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_metric(self,\n",
    "                  reset: bool = False) -> Dict[str,float]:\n",
    "        \n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Instantiate the model\n",
    "\n",
    " * must pass to the model \n",
    "   * Vocabulary of dataset\n",
    "     * to get the size of label vocabulary (== output feature dimension)    \n",
    "   * type of Embedder to use\n",
    "   * type of Encoder to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Vocabulary\n",
    "# 1. Define vocabulary\n",
    "vocab = Vocabulary.from_instances(train_dataset)\n",
    "vocabulary_size = vocab.get_vocab_size('tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define embedders\n",
    "from allennlp.modules import Embedding\n",
    "\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "vocabulary_size = input_vocab.get_vocab_size('tokens')\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocabulary_size,\n",
    "                            embedding_dim = 6)\n",
    "\n",
    "word_embeddings = BasicTextFieldEmbedder({'tokens':token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define encoder \n",
    "\n",
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\n",
    "\n",
    "module = torch.nn.LSTM(6,6,batch_first=True)\n",
    "\n",
    "lstm_encoder = PytorchSeq2SeqWrapper(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally instantiate the model\n",
    "\n",
    "model = LstmTagger(word_embeddings,lstm_encoder ,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Trainer\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "iterator = BucketIterator(batch_size=2,sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "trainer = Trainer(model = model,\n",
    "                  optimizer = optimizer,\n",
    "                  iterator = iterator,\n",
    "                  train_dataset = train_dataset,\n",
    "                  validation_dataset = validation_dataset,\n",
    "                  patience = 10,\n",
    "                  num_epochs = 1000)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE : \n",
    "raw_train_generator = iterator(train_dataset,num_epochs = 10, shuffle=True)\n",
    "\n",
    "for batch in raw_train_generator:\n",
    "    print(batch)\n",
    "    \n",
    "# Dict[str,torch.Tensor]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.  Prediction (Verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import SentenceTaggerPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = SentenceTaggerPredictor(model, dataset_reader = reader)\n",
    "\n",
    "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "\n",
    "print (\"The dog ate the apple\")\n",
    "print([model.vocab.get_token_from_index(i,'labels') for i in tag_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
